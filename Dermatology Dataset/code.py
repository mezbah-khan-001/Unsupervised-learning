# -*- coding: utf-8 -*-
"""Dermatology Dataset.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1STJn6v4eTUOBZRw2LOi6uUfNreDVXvWf
"""

#Hello Wrold this is Ryo
 #Lets code the Program  ---->
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import sklearn as skn
from pathlib import Path
from icecream import ic
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import optuna
import time
import warnings

# Suppress warnings
warnings.filterwarnings("ignore")

# Load the dataset
data_path = Path('/content/dermatology_database_1.csv')
if data_path.exists():
    data = pd.read_csv(data_path)
    ic('Data loaded successfully...')
else:
    raise FileNotFoundError(f'This file path {data_path} does not exist...')

# Clean the 'age' column
data['age'] = data['age'].astype(str).str.replace(r'[^0-9]', '', regex=True)  # Remove all non-digits
data['age'] = pd.to_numeric(data['age'], errors='coerce')  # Convert to float, NaNs if invalid

# Impute missing values with the median
median_age = data['age'].median()
data['age'].fillna(median_age, inplace=True)

# Convert age to 'int64' after imputation
data['age'] = data['age'].astype('int64')

# Check for missing values
ic(data['age'].isnull().sum())  # Should be 0 (no NaNs)

# Label encode the target (class column)
le = LabelEncoder()
data['class'] = le.fit_transform(data['class'])

# Split features and target
X = data.drop(columns=['class'])
y = data['class']

# Feature scaling using StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the dataset into training and testing sets (80/20 ratio)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Define the model (RandomForestClassifier)
model = RandomForestClassifier(random_state=42)

# Train the model
start_time = time.time()
model.fit(X_train, y_train)
train_time = time.time() - start_time

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
f1_report = classification_report(y_test, y_pred)

# Output evaluation metrics
ic(f"Training Time: {train_time:.2f} seconds")
ic(f"Accuracy: {accuracy:.4f}")
ic(f"Confusion Matrix:\n{conf_matrix}")
ic(f"F1 Score Report:\n{f1_report}")

# Optional: Hyperparameter tuning using Optuna
def objective(trial):
    # Define hyperparameters to optimize
    n_estimators = trial.suggest_int('n_estimators', 50, 300)
    max_depth = trial.suggest_int('max_depth', 5, 50)
    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)

    # Create and train model with the selected hyperparameters
    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                   min_samples_split=min_samples_split, random_state=42)

    model.fit(X_train, y_train)

    # Evaluate the model
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    return accuracy

# Create Optuna study to optimize the model
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=10)

# Output the best hyperparameters and accuracy
ic(f"Best Hyperparameters: {study.best_params}")
ic(f"Best Accuracy: {study.best_value:.4f}")

# After optimization, train the model with the best parameters
best_params = study.best_params
optimized_model = RandomForestClassifier(n_estimators=best_params['n_estimators'],
                                         max_depth=best_params['max_depth'],
                                         min_samples_split=best_params['min_samples_split'],
                                         random_state=42)
optimized_model.fit(X_train, y_train)

# Final evaluation with the optimized model
y_pred_optimized = optimized_model.predict(X_test)
final_accuracy = accuracy_score(y_test, y_pred_optimized)
final_conf_matrix = confusion_matrix(y_test, y_pred_optimized)
final_f1_report = classification_report(y_test, y_pred_optimized)

# Output the final evaluation metrics
ic(f"Optimized Model Accuracy: {final_accuracy:.4f}")
ic(f"Optimized Model Confusion Matrix:\n{final_conf_matrix}")
ic(f"Optimized Model F1 Score Report:\n{final_f1_report}")

# Optionally, save the model for deployment (e.g., using joblib or pickle)
import joblib
joblib.dump(optimized_model, '/content/optimized_random_forest_model.pkl')
ic("Model saved successfully for deployment!")
model_path = r'C:\Projects\Dermatology Dataset\model_filename.pkl'
joblib.dump(model, model_path)

# Verify that the model is saved
print(f"Model saved at: {model_path}")